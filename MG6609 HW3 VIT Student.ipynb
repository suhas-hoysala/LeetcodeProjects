{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "In this assignment we're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task.\n",
        "The purpose of this homework is for you to get familar with ViT and get prepared for the final project."
      ],
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xGv2wu1MyAPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fffe2d-3a94-41f4-e989-419511860a92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "For the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\n",
        "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ],
      "metadata": {
        "id": "MmNi93C-4rLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ],
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B x C x H x W -> B x C x N where N = H*W/patch_size^2\n",
        "        x = self.projection(x)  # B x embed_dim x H/patch_size x W/patch_size\n",
        "        # B x embed_dim x (H*W/patch_size^2) -> B x (H*W/patch_size^2) x embed_dim\n",
        "        x = x.flatten(2)  # B x embed_dim x N\n",
        "        x = x.transpose(1, 2)  # B x N x embed_dim\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rAzsdK5YybDa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ],
      "metadata": {
        "id": "1mk8v66y6MAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Initialize the weights of the attention layers using Xavier uniform initialization\n",
        "        # This helps with training stability by keeping the variance of activations roughly constant\n",
        "        # across layers at the start of training\n",
        "        # Xavier initialization is particularly well-suited for tanh activations and similar\n",
        "        # The weights are initialized with Xavier uniform while biases are set to 0\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)  # Initialize query/key/value projection weights\n",
        "        self.qkv_proj.bias.data.fill_(0)  # Initialize query/key/value projection biases\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)  # Initialize output projection weights\n",
        "        self.o_proj.bias.data.fill_(0)  # Initialize output projection biases\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get input dimensions\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Project input into query, key and value vectors all at once\n",
        "        # Output shape: (batch_size, seq_length, 3*embed_dim)\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Reshape and permute to separate heads and split QKV\n",
        "        # First reshape to: (batch_size, seq_length, num_heads, 3*head_dim)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        # Permute to: (batch_size, num_heads, seq_length, 3*head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        # Split into query, key, value tensors along last dimension\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        # values shape: (batch_size, num_heads, seq_length, head_dim)\n",
        "        # _attention contains the attention weights but we don't use them here\n",
        "        values, _attention = scaled_dot_product(q, k, v)\n",
        "\n",
        "        # Reshape attention output\n",
        "        # First permute back to: (batch_size, seq_length, num_heads, head_dim)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        # Then combine heads: (batch_size, seq_length, embed_dim)\n",
        "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "\n",
        "        # Final linear projection\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        return o\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V1LeAZq-0dQW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "You may also want to use layer normalization or other type of normalization."
      ],
      "metadata": {
        "id": "NCAURJGJ6jhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self attention layer\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        # Layer normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # MLP block\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First sublayer: Multi-head self attention with residual connection\n",
        "        attention_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attention_out)\n",
        "\n",
        "        # Second sublayer: MLP with residual connection\n",
        "        mlp_out = self.mlp(self.norm2(x))\n",
        "        x = x + self.dropout(mlp_out)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VisionTransformer:\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ],
      "metadata": {
        "id": "rgLfJRUm7EDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding layer\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        # Calculate number of patches\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        # Class token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Positional embedding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create patch embeddings\n",
        "        x = self.patch_embed(x)  # Shape: (batch_size, num_patches, embed_dim)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Layer normalization\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Use the class token for classification\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # Classification head\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
      ],
      "metadata": {
        "id": "lROdKoO37Uqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hypers\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 1032 #1024 #768\n",
        "num_heads = 12\n",
        "mlp_dim = 4096 #3072\n",
        "num_layers = 12\n",
        "num_classes = 100\n",
        "dropout = 0.1\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# faster loading\n",
        "num_workers = 4\n"
      ],
      "metadata": {
        "id": "byAC841ix_lb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "1V14TFbM8x4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb12cdc-a327-48b8-d541-c7999b1e5149"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
      ],
      "metadata": {
        "id": "3BOp450mdC-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458910ae-0566-4bab-b232-e6a662ffaa8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:12<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.001,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=len(trainloader),\n",
        "    pct_start=0.3,  # Spend 30% of time ramping up, 70% ramping down\n",
        ")\n",
        "\n",
        "# more\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "4s8-X4l-exSg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "best_val_acc = 0\n",
        "start_epoch = 0\n",
        "\n",
        "# Try to load checkpoint if exists\n",
        "checkpoint_path = \"training_checkpoint.pth\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_val_acc = checkpoint['best_val_acc']\n",
        "    print(f\"Resuming from epoch {start_epoch} with best validation accuracy: {best_val_acc:.2f}%\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting new.\")\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    train_pbar = tqdm.tqdm(trainloader, desc=f'Training Epoch {epoch+1}/{num_epochs}')\n",
        "    for i, data in enumerate(train_pbar):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        val_pbar = tqdm.tqdm(testloader, desc=f'Validation Epoch {epoch+1}/{num_epochs}')\n",
        "        for data in val_pbar:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            val_pbar.set_postfix({'val_loss': f'{torch.tensor(val_losses).mean().item():.4f}'})\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_acc': best_val_acc,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # Save the best model separately\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "eOyk345ve5HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2df7625-a4f9-4842-f7b9-4bf1ed455c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found. Starting new.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 391/391 [09:57<00:00,  1.53s/it, loss=3.4577]\n",
            "Validation Epoch 1/10: 100%|██████████| 79/79 [00:39<00:00,  1.99it/s, val_loss=3.6654]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Validation Accuracy: 13.33%\n",
            "New best model saved with validation accuracy: 13.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 391/391 [09:56<00:00,  1.52s/it, loss=3.2250]\n",
            "Validation Epoch 2/10: 100%|██████████| 79/79 [00:39<00:00,  1.99it/s, val_loss=3.1946]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Validation Accuracy: 21.74%\n",
            "New best model saved with validation accuracy: 21.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 391/391 [09:55<00:00,  1.52s/it, loss=3.2133]\n",
            "Validation Epoch 3/10: 100%|██████████| 79/79 [00:39<00:00,  2.00it/s, val_loss=2.8976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Validation Accuracy: 27.89%\n",
            "New best model saved with validation accuracy: 27.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 391/391 [09:55<00:00,  1.52s/it, loss=3.0310]\n",
            "Validation Epoch 4/10: 100%|██████████| 79/79 [00:39<00:00,  2.00it/s, val_loss=2.7330]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Validation Accuracy: 31.06%\n",
            "New best model saved with validation accuracy: 31.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10:  48%|████▊     | 189/391 [04:50<05:07,  1.52s/it, loss=2.5591]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit your best_model.pth with this notebook. And report the best test results you get."
      ],
      "metadata": {
        "id": "-AfNVj1U9xhk"
      }
    }
  ]
}